{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1063445,"sourceType":"datasetVersion","datasetId":589983}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Projeto: Segmentação de Vasos Sanguíneos da Retina com **U‑Net++**\n\n> Guia completo para implementar e treinar um modelo **U‑Net++** no dataset **DRIVE**, capaz de detectar com precisão vasos sanguíneos em imagens de fundo de olho ─ tarefa fundamental no diagnóstico precoce de doenças como a retinopatia diabética.\n\n---\n\n## 1  ·  Visão Geral e Objetivos\n| Item          | Escolha |\n|---------------|---------|\n| **Dataset**   | DRIVE (Digital Retinal Images for Vessel Extraction) |\n| **Modelo**    | U‑Net++ |\n| **Framework** | TensorFlow / Keras |\n| **Ambiente**  | Kaggle Notebook com GPU (T4 × 2) |\n\n*Desafio:* Criar um pipeline de **deep learning** que segmente vasos sanguíneos com alta sensibilidade e especificidade.\n\n---\n\n## 2. Compreendendo a U-Net++\n\nAntes de codificar, é crucial entender por que a U-Net++ é uma melhoria em relação à U-Net original.\n\nA U-Net introduziu as skip connections (conexões de atalho), que combinam características de mapas de alta resolução (do codificador) com mapas de características reamostrados (do decodificador). Isso ajuda a recuperar detalhes espaciais perdidos durante o downsampling.\n\nA U-Net++ aprimora isso com duas inovações principais:\n\n- **Conexões de Atalho Aninhadas e Densas (Nested and Dense Skip Connections):** Em vez de uma única conexão longa, a U-Net++ insere nós de convolução ao longo das skip connections. Isso cria um caminho mais gradual para o fluxo de gradientes e preenche a \"lacuna semântica\" entre os mapas de características do codificador e do decodificador. Os mapas de características que chegam ao decodificador são semanticamente mais ricos.\n\n- **Supervisão Profunda (Deep Supervision):** O modelo pode ser treinado para gerar saídas em múltiplas resoluções. As saídas dos nós X_0,1, X_0,2, X_0,3 e X_0,4 (na notação do paper) podem ser usadas para calcular uma perda combinada. Isso força os nós intermediários a aprenderem características úteis, resultando em uma otimização mais robusta e a capacidade de \"podar\" o modelo em tempo de inferência para um equilíbrio entre velocidade e precisão.\n\n\n## 3. Configuração do Ambiente Kaggle\n\n\nNa primeira célula, vamos instalar e importar todas as bibliotecas necessárias e carregar o dataset usando kagglehub.","metadata":{}},{"cell_type":"code","source":"# Instalação do Kaggle Hub se não estiver presente\n!pip install -q kagglehub\n\n# Importações essenciais\nimport os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2 # OpenCV para algumas operações de pré-processamento\nfrom sklearn.model_selection import train_test_split\n\n# Import para carregar o dataset\nimport kagglehub\n\n# Silenciar logs desnecessários\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:56:37.327528Z","iopub.execute_input":"2025-06-20T16:56:37.327842Z","iopub.status.idle":"2025-06-20T16:56:40.380799Z","shell.execute_reply.started":"2025-06-20T16:56:37.327809Z","shell.execute_reply":"2025-06-20T16:56:40.379727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download e descompactação do dataset\n# O Kaggle gerencia o caminho automaticamente\ndataset_path = kagglehub.dataset_download(\"andrewmvd/drive-digital-retinal-images-for-vessel-extraction\")\n\nprint(f\"Dataset baixado em: {dataset_path}\")\n\n# Definir os caminhos principais baseados na estrutura do dataset\nDRIVE_DIR = os.path.join(dataset_path, \"DRIVE\")\nTRAIN_IMG_DIR = os.path.join(DRIVE_DIR, \"training/images\")\nTRAIN_MASK_DIR = os.path.join(DRIVE_DIR, \"training/1st_manual\") # Usaremos a anotação manual como ground truth\nTRAIN_FOV_DIR = os.path.join(DRIVE_DIR, \"training/mask\") # Máscaras do campo de visão (FOV)\n\nTEST_IMG_DIR = os.path.join(DRIVE_DIR, \"test/images\")\nTEST_MASK_DIR = os.path.join(DRIVE_DIR, \"test/1st_manual\") # Disponível no Kaggle, mas vamos simular um teste real\nTEST_FOV_DIR = os.path.join(DRIVE_DIR, \"test/mask\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:56:40.382811Z","iopub.execute_input":"2025-06-20T16:56:40.383068Z","iopub.status.idle":"2025-06-20T16:56:40.518934Z","shell.execute_reply.started":"2025-06-20T16:56:40.383046Z","shell.execute_reply":"2025-06-20T16:56:40.518223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pastas do dataset\n```\nDRIVE/\n├── training/\n│   ├── mask/                       # Contém máscaras de treinamento em formato GIF\n│   │   ├── 21_training_mask.gif\n│   │   ├── 22_training_mask.gif\n│   │   └── ... (até 40_training_mask.gif)\n│   ├── images/                     # Contém imagens de treinamento em formato TIF\n│   │   ├── 21_training.tif\n│   │   ├── 22_training.tif\n│   │   └── ... (até 40_training.tif)\n│   └── 1st_manual/                 # Contém anotações manuais em formato GIF\n│       ├── 21_manual1.gif\n│       ├── 22_manual1.gif\n│       └── ... (até 40_manual1.gif)\n└── test/\n    ├── mask/                       # Contém máscaras de teste em formato GIF\n    │   ├── 01_test_mask.gif\n    │   ├── 02_test_mask.gif\n    │   └── ... (até 20_test_mask.gif)\n    └── images/                     # Contém imagens de teste em formato TIF\n        ├── 01_test.tif\n        ├── 02_test.tif\n        └── ... (até 20_test.tif)\n```\n","metadata":{}},{"cell_type":"markdown","source":"## 4  ·  Carregamento e Pré-processamento dos Dados\n\nImagens médicas requerem um pré-processamento criterioso. Isso inclui normalização, redimensionamento e, opcionalmente, divisão em **patches** (recortes menores), o que ajuda a aumentar a eficiência e a variabilidade do treinamento.\n\n---\n\n### 4.1  ·  Funções de Carregamento\n\nA seguir, criaremos uma função que:\n\n- Lê imagens `.tif` e máscaras `.gif`\n- Converte para arrays NumPy\n- Normaliza os valores para o intervalo **[0, 1]**","metadata":{}},{"cell_type":"code","source":"def load_data(img_dir, mask_dir, fov_dir):\n    \"\"\"Carrega imagens, máscaras e máscaras de FOV.\"\"\"\n    img_files = sorted(glob.glob(os.path.join(img_dir, \"*.tif\")))\n    mask_files = sorted(glob.glob(os.path.join(mask_dir, \"*.gif\")))\n    fov_files = sorted(glob.glob(os.path.join(fov_dir, \"*.gif\")))\n\n    images = []\n    masks = []\n    fov_masks = []\n\n    for img_path, mask_path, fov_path in zip(img_files, mask_files, fov_files):\n        # Carregar imagem e normalizar\n        img = Image.open(img_path).convert('RGB')\n        img = np.array(img, dtype=np.float32) / 255.0\n        images.append(img)\n\n        # Carregar máscara e binarizar\n        mask = Image.open(mask_path)\n        mask = np.array(mask, dtype=np.float32)\n        mask[mask > 0] = 1.0 # Garantir que seja 0 ou 1\n        mask = np.expand_dims(mask, axis=-1) # Adicionar canal\n        masks.append(mask)\n\n        # Carregar máscara de FOV e binarizar\n        fov = Image.open(fov_path)\n        fov = np.array(fov, dtype=np.float32)\n        fov[fov > 0] = 1.0\n        fov = np.expand_dims(fov, axis=-1)\n        fov_masks.append(fov)\n\n    return np.array(images), np.array(masks), np.array(fov_masks)\n\n# Carregar os dados de treinamento\ntrain_images, train_masks, train_fov_masks = load_data(TRAIN_IMG_DIR, TRAIN_MASK_DIR, TRAIN_FOV_DIR)\n\nprint(f\"Imagens de treinamento carregadas: {train_images.shape}\")\nprint(f\"Máscaras de treinamento carregadas: {train_masks.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:56:40.519565Z","iopub.execute_input":"2025-06-20T16:56:40.519755Z","iopub.status.idle":"2025-06-20T16:56:41.018805Z","shell.execute_reply.started":"2025-06-20T16:56:40.519741Z","shell.execute_reply":"2025-06-20T16:56:41.01803Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n### 4.2  ·  Criação de Patches (Recortes)\n\nAs imagens originais (584×565) são grandes demais para caber na memória da GPU de uma só vez.  \nA técnica padrão é extrair **patches** menores e treinar o modelo com eles.\n","metadata":{}},{"cell_type":"code","source":"def create_patches(images, masks, patch_size=128, stride=64):\n    \"\"\"Extrai patches sobrepostos das imagens e máscaras.\"\"\"\n    img_patches = []\n    mask_patches = []\n\n    img_h, img_w = images.shape[1], images.shape[2]\n\n    for i in range(images.shape[0]): # Para cada imagem\n        for y in range(0, img_h - patch_size + 1, stride):\n            for x in range(0, img_w - patch_size + 1, stride):\n                img_patch = images[i, y:y+patch_size, x:x+patch_size]\n                mask_patch = masks[i, y:y+patch_size, x:x+patch_size]\n                \n                # Adicionar apenas patches que contenham alguma informação de vaso\n                if np.sum(mask_patch) > 0:\n                    img_patches.append(img_patch)\n                    mask_patches.append(mask_patch)\n\n    return np.array(img_patches), np.array(mask_patches)\n\n# Criar patches para treinamento e validação\n# Usaremos um split simples para criar um conjunto de validação\nX_train, X_val, y_train, y_val = train_test_split(train_images, train_masks, test_size=0.1, random_state=42)\n\ntrain_img_patches, train_mask_patches = create_patches(X_train, y_train)\nval_img_patches, val_mask_patches = create_patches(X_val, y_val)\n\nprint(f\"Patches de imagem de treinamento: {train_img_patches.shape}\")\nprint(f\"Patches de máscara de treinamento: {train_mask_patches.shape}\")\nprint(f\"Patches de imagem de validação: {val_img_patches.shape}\")\nprint(f\"Patches de máscara de validação: {val_mask_patches.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:56:41.020555Z","iopub.execute_input":"2025-06-20T16:56:41.020786Z","iopub.status.idle":"2025-06-20T16:56:41.151667Z","shell.execute_reply.started":"2025-06-20T16:56:41.020769Z","shell.execute_reply":"2025-06-20T16:56:41.150883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Definir o número de exemplos que queremos visualizar\nnum_samples_to_show = 3\n\nprint(\"Exibindo exemplos de imagens de treino e suas máscaras correspondentes...\")\n\n# Criar uma figura para cada par de imagem/máscara\nfor i in range(num_samples_to_show):\n    \n    # Criar uma nova figura com tamanho adequado\n    plt.figure(figsize=(12, 5))\n    \n    # Plotar a Imagem Original\n    plt.subplot(1, 2, 1)\n    plt.title(f\"Imagem de Treino #{i+1}\")\n    plt.imshow(X_train[i]) # X_train contém as imagens originais\n    plt.axis('off') # Remover os eixos para uma visualização mais limpa\n    \n    # Plotar a Máscara (Gabarito)\n    plt.subplot(1, 2, 2)\n    plt.title(f\"Máscara (Gabarito) #{i+1}\")\n    plt.imshow(y_train[i], cmap='gray') # y_train contém as máscaras. 'cmap=gray' garante a exibição em preto e branco.\n    plt.axis('off')\n    \n    # Mostrar o par de plots\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:56:41.152633Z","iopub.execute_input":"2025-06-20T16:56:41.152931Z","iopub.status.idle":"2025-06-20T16:56:41.982386Z","shell.execute_reply.started":"2025-06-20T16:56:41.152908Z","shell.execute_reply":"2025-06-20T16:56:41.981692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 5  ·  Implementação da U‑Net++\n\nAgora, a parte central: **construir o modelo**.  \nVamos criar um **bloco de convolução padrão** e depois montar a arquitetura **U‑Net++** completa.\n","metadata":{}},{"cell_type":"code","source":"# Bloco de convolução padrão\ndef conv_block(inputs, num_filters):\n    x = layers.Conv2D(num_filters, 3, padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    \n    x = layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    \n    return x\n\n# Função para construir o modelo U-Net++\ndef build_unet_plus_plus(input_shape, deep_supervision=False):\n    \"\"\"\n    Constrói a arquitetura U-Net++.\n    Notação: X_i_j onde i é o nível de downsampling e j é a camada de convolução no skip path.\n    \"\"\"\n    inputs = layers.Input(shape=input_shape)\n    \n    # --- Codificador ---\n    X_0_0 = conv_block(inputs, 32)\n    p0 = layers.MaxPooling2D(2)(X_0_0)\n    \n    X_1_0 = conv_block(p0, 64)\n    p1 = layers.MaxPooling2D(2)(X_1_0)\n\n    X_2_0 = conv_block(p1, 128)\n    p2 = layers.MaxPooling2D(2)(X_2_0)\n    \n    X_3_0 = conv_block(p2, 256)\n    p3 = layers.MaxPooling2D(2)(X_3_0)\n    \n    X_4_0 = conv_block(p3, 512) # Camada bottleneck\n\n    # --- Skip Pathways Aninhados e Densos ---\n    \n    # Nível 1\n    u1_0 = layers.Conv2DTranspose(64, 2, strides=2, padding=\"same\")(X_1_0)\n    X_0_1 = conv_block(layers.concatenate([X_0_0, u1_0]), 32)\n    \n    # Nível 2\n    u2_0 = layers.Conv2DTranspose(128, 2, strides=2, padding=\"same\")(X_2_0)\n    X_1_1 = conv_block(layers.concatenate([X_1_0, u2_0]), 64)\n\n    u1_1 = layers.Conv2DTranspose(32, 2, strides=2, padding=\"same\")(X_1_1)\n    X_0_2 = conv_block(layers.concatenate([X_0_0, X_0_1, u1_1]), 32)\n    \n    # Nível 3\n    u3_0 = layers.Conv2DTranspose(256, 2, strides=2, padding=\"same\")(X_3_0)\n    X_2_1 = conv_block(layers.concatenate([X_2_0, u3_0]), 128)\n    \n    u2_1 = layers.Conv2DTranspose(64, 2, strides=2, padding=\"same\")(X_2_1)\n    X_1_2 = conv_block(layers.concatenate([X_1_0, X_1_1, u2_1]), 64)\n    \n    u1_2 = layers.Conv2DTranspose(32, 2, strides=2, padding=\"same\")(X_1_2)\n    X_0_3 = conv_block(layers.concatenate([X_0_0, X_0_1, X_0_2, u1_2]), 32)\n    \n    # Nível 4 (Decoder)\n    u4_0 = layers.Conv2DTranspose(512, 2, strides=2, padding=\"same\")(X_4_0)\n    X_3_1 = conv_block(layers.concatenate([X_3_0, u4_0]), 256)\n    \n    u3_1 = layers.Conv2DTranspose(128, 2, strides=2, padding=\"same\")(X_3_1)\n    X_2_2 = conv_block(layers.concatenate([X_2_0, X_2_1, u3_1]), 128)\n    \n    u2_2 = layers.Conv2DTranspose(64, 2, strides=2, padding=\"same\")(X_2_2)\n    X_1_3 = conv_block(layers.concatenate([X_1_0, X_1_1, X_1_2, u2_2]), 64)\n    \n    u1_3 = layers.Conv2DTranspose(32, 2, strides=2, padding=\"same\")(X_1_3)\n    X_0_4 = conv_block(layers.concatenate([X_0_0, X_0_1, X_0_2, X_0_3, u1_3]), 32)\n\n    # --- Saídas ---\n    output1 = layers.Conv2D(1, 1, activation=\"sigmoid\", name=\"output_1\")(X_0_1)\n    output2 = layers.Conv2D(1, 1, activation=\"sigmoid\", name=\"output_2\")(X_0_2)\n    output3 = layers.Conv2D(1, 1, activation=\"sigmoid\", name=\"output_3\")(X_0_3)\n    output4 = layers.Conv2D(1, 1, activation=\"sigmoid\", name=\"output_4\")(X_0_4)\n\n    if deep_supervision:\n        return keras.Model(inputs, [output1, output2, output3, output4])\n    else:\n        return keras.Model(inputs, output4)\n\n# Definir parâmetros e construir o modelo\nPATCH_SIZE = 128\nCHANNELS = 3\nINPUT_SHAPE = (PATCH_SIZE, PATCH_SIZE, CHANNELS)\nDEEP_SUPERVISION = True # Ativar a supervisão profunda\n\nmodel = build_unet_plus_plus(INPUT_SHAPE, deep_supervision=DEEP_SUPERVISION)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:56:41.983694Z","iopub.execute_input":"2025-06-20T16:56:41.984219Z","iopub.status.idle":"2025-06-20T16:56:42.593357Z","shell.execute_reply.started":"2025-06-20T16:56:41.984197Z","shell.execute_reply":"2025-06-20T16:56:42.592649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n## 6. Treinamento do Modelo\n\n\n### 6.1. Função de Perda e Métricas\n\nPara segmentação, uma combinação da perda Binary Cross-Entropy (BCE) e Dice Loss é muito eficaz. A BCE trata a classificação de cada pixel de forma independente, enquanto a Dice Loss avalia a sobreposição entre a predição e a máscara real, lidando bem com o desbalanceamento de classes (muito mais pixels de fundo do que de vasos).","metadata":{}},{"cell_type":"code","source":"# %% Data Augmentation (Aumento de Dados)\n\n\nBATCH_SIZE = 16\nBUFFER_SIZE = len(train_img_patches) # Buffer para embaralhamento\n\ndef augment_data(image, mask):\n    \"\"\"\n    Aplica transformações de aumento de dados em um par imagem-máscara.\n    \"\"\"\n    # Empilhar imagem e máscara para aplicar a mesma transformação geométrica\n    mask_float = tf.cast(mask, tf.float32)\n    combined = tf.concat([image, mask_float], axis=-1)\n\n    # Transformações Geométricas\n    combined = tf.image.random_flip_left_right(combined)\n    combined = tf.image.random_flip_up_down(combined)\n\n    # Desempilhar a imagem e a máscara\n    image = combined[:, :, :3]\n    mask = combined[:, :, 3:]\n    \n    # Garantir que a máscara volte a ser binária\n    mask = tf.cast(mask > 0.5, tf.float32)\n\n    # Transformações de Cor (apenas na imagem)\n    image = tf.image.random_brightness(image, max_delta=0.1)\n    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n    \n    # Garantir que a imagem permaneça no intervalo [0, 1]\n    image = tf.clip_by_value(image, 0.0, 1.0)\n\n    return image, mask\n\ndef format_targets_for_deep_supervision(image, mask):\n    \"\"\"\n    Formata a máscara para o dicionário esperado pelo modelo.\n    \"\"\"\n    targets = {\n        \"output_1\": mask,\n        \"output_2\": mask,\n        \"output_3\": mask,\n        \"output_4\": mask,\n    }\n    return image, targets\n\n# Criar o pipeline de dados de TREINAMENTO com aumento de dados\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_img_patches, train_mask_patches))\ntrain_dataset = (\n    train_dataset.cache()\n    .shuffle(BUFFER_SIZE)\n    .map(augment_data, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n\n# Criar o pipeline de dados de VALIDAÇÃO (sem aumento de dados)\nval_dataset = tf.data.Dataset.from_tensor_slices((val_img_patches, val_mask_patches))\nval_dataset = (\n    val_dataset\n    .batch(BATCH_SIZE)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n\n# Formatar os targets para supervisão profunda, se necessário\nif DEEP_SUPERVISION:\n    train_dataset = train_dataset.map(format_targets_for_deep_supervision)\n    val_dataset = val_dataset.map(format_targets_for_deep_supervision)\n\nprint(\"Pipelines tf.data com aumento de dados foram criados com sucesso!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:56:42.594384Z","iopub.execute_input":"2025-06-20T16:56:42.594663Z","iopub.status.idle":"2025-06-20T16:56:43.376523Z","shell.execute_reply.started":"2025-06-20T16:56:42.594639Z","shell.execute_reply":"2025-06-20T16:56:43.37588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Métrica de Dice Coefficient\ndef dice_coefficient(y_true, y_pred, smooth=1e-6):\n    y_true_f = tf.cast(tf.reshape(y_true, [-1]), tf.float32)\n    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]), tf.float32)\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\n# Função de perda (Dice Loss)\ndef dice_loss(y_true, y_pred):\n    return 1 - dice_coefficient(y_true, y_pred)\n\n# Perda combinada\ndef bce_dice_loss(y_true, y_pred):\n    return keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:56:43.377194Z","iopub.execute_input":"2025-06-20T16:56:43.377398Z","iopub.status.idle":"2025-06-20T16:56:43.382772Z","shell.execute_reply.started":"2025-06-20T16:56:43.377383Z","shell.execute_reply":"2025-06-20T16:56:43.381979Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.2. Compilação e Treinamento\nVamos compilar o modelo e iniciar o treinamento. Com a supervisão profunda, precisamos fornecer a máscara de treinamento para cada uma das quatro saídas do modelo.","metadata":{}},{"cell_type":"code","source":"# Configurar o otimizador e a compilação\noptimizer = keras.optimizers.Adam(learning_rate=1e-4)\n\nif DEEP_SUPERVISION:\n    # A perda é aplicada a cada saída e somada\n    losses = {\n        \"output_1\": bce_dice_loss,\n        \"output_2\": bce_dice_loss,\n        \"output_3\": bce_dice_loss,\n        \"output_4\": bce_dice_loss,\n    }\n    # Ponderar as perdas (a saída final é mais importante)\n    loss_weights = {\"output_1\": 0.25, \"output_2\": 0.25,\n                    \"output_3\": 0.25, \"output_4\": 1.0}\n\n    # As métricas serão monitoradas apenas na saída final\n    model.compile(optimizer=optimizer,\n                  loss=losses,\n                  loss_weights=loss_weights,\n                  metrics={\"output_4\": [dice_coefficient, \"accuracy\"]})\n\nelse:\n    model.compile(optimizer=optimizer,\n                  loss=bce_dice_loss,\n                  metrics=[dice_coefficient, \"accuracy\"])\n\n# Callbacks para um treinamento mais robusto\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\n        \"unetpp_drive.keras\", save_best_only=True, monitor=\"val_output_4_dice_coefficient\", mode='max'),\n    keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_output_4_loss\", factor=0.1, patience=5, min_lr=1e-6, verbose=1),\n    keras.callbacks.EarlyStopping(\n        monitor=\"val_output_4_loss\", patience=10, restore_best_weights=True, mode='min')\n]\n\n# Treinamento (NOVA CHAMADA model.fit)\nEPOCHS = 60\n\nhistory = model.fit(\n    train_dataset,  # USA O NOVO PIPELINE DE TREINO\n    epochs=EPOCHS,\n    validation_data=val_dataset, # USA O NOVO PIPELINE DE VALIDAÇÃO\n    callbacks=callbacks\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T16:56:43.383503Z","iopub.execute_input":"2025-06-20T16:56:43.38371Z","iopub.status.idle":"2025-06-20T17:15:14.284412Z","shell.execute_reply.started":"2025-06-20T16:56:43.383688Z","shell.execute_reply":"2025-06-20T17:15:14.283752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Avaliação do Modelo\nApós o treinamento, vamos avaliar o desempenho do modelo no conjunto de teste. A avaliação será feita nas imagens completas, o que requer a reconstrução das predições a partir dos patches.\n\n### 7.1. Reconstrução da Imagem e Métricas","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, roc_auc_score\n\ndef reconstruct_from_patches(patches, original_img_shape, stride=64):\n    img_h, img_w = original_img_shape[0], original_img_shape[1]\n    patch_size = patches.shape[1]\n    \n    reconstructed_img = np.zeros((img_h, img_w, 1))\n    count_matrix = np.zeros((img_h, img_w, 1))\n    \n    patch_idx = 0\n    for y in range(0, img_h - patch_size + 1, stride):\n        for x in range(0, img_w - patch_size + 1, stride):\n            if patch_idx < len(patches):\n                reconstructed_img[y:y+patch_size, x:x+patch_size] += patches[patch_idx]\n                count_matrix[y:y+patch_size, x:x+patch_size] += 1\n                patch_idx += 1\n            \n    count_matrix[count_matrix == 0] = 1\n    final_image = reconstructed_img / count_matrix\n    return final_image\n\ndef evaluate_model(model, test_images, test_masks, test_fov_masks, patch_size=128, stride=64):\n    all_metrics = {\"dice\": [], \"accuracy\": [], \"sensitivity\": [], \"specificity\": [], \"auc\": []}\n    \n    for i in range(len(test_images)):\n        img = test_images[i]\n        true_mask = test_masks[i]\n        fov_mask = test_fov_masks[i].astype(bool).flatten()\n\n        # Gerando todos os patches da imagem de teste, sem pular nenhum.\n        img_h, img_w = img.shape[0], img.shape[1]\n        test_patches_list = []\n        for y in range(0, img_h - patch_size + 1, stride):\n            for x in range(0, img_w - patch_size + 1, stride):\n                patch = img[y:y+patch_size, x:x+patch_size]\n                test_patches_list.append(patch)\n        \n        test_patches = np.array(test_patches_list)\n\n        if len(test_patches) == 0:\n            print(f\"Aviso: Nenhum patch gerado para a imagem {i}, pulando.\")\n            continue\n\n        if DEEP_SUPERVISION:\n            preds = model.predict(test_patches, verbose=0)[-1]\n        else:\n            preds = model.predict(test_patches, verbose=0)\n\n        pred_mask = reconstruct_from_patches(preds, img.shape, stride=stride)\n        pred_mask_flat = pred_mask.flatten()\n        pred_mask_bin = (pred_mask > 0.5).astype(np.uint8)\n        pred_mask_bin_flat = pred_mask_bin.flatten()\n        true_mask_flat = true_mask.flatten()\n        true_mask_fov = true_mask_flat[fov_mask]\n        pred_mask_bin_fov = pred_mask_bin_flat[fov_mask]\n        pred_mask_fov = pred_mask_flat[fov_mask]\n\n        if len(np.unique(true_mask_fov)) < 2:\n            print(f\"Aviso: Máscara da imagem {i} não tem ambas as classes após filtro FOV. Pulando.\")\n            continue\n        \n        tn, fp, fn, tp = confusion_matrix(true_mask_fov, pred_mask_bin_fov, labels=[0, 1]).ravel()\n        \n        dice = (2. * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n        sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n        spec = tn / (tn + fp) if (tn + fp) > 0 else 0\n        acc = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n\n        all_metrics[\"dice\"].append(dice)\n        all_metrics[\"accuracy\"].append(acc)\n        all_metrics[\"sensitivity\"].append(sens)\n        all_metrics[\"specificity\"].append(spec)\n        all_metrics[\"auc\"].append(roc_auc_score(true_mask_fov, pred_mask_fov))\n\n    avg_metrics = {key: np.mean(val) for key, val in all_metrics.items()}\n    return avg_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:15:38.289264Z","iopub.execute_input":"2025-06-20T17:15:38.289863Z","iopub.status.idle":"2025-06-20T17:15:38.309735Z","shell.execute_reply.started":"2025-06-20T17:15:38.289813Z","shell.execute_reply":"2025-06-20T17:15:38.309009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Recarregando os dados de treino completos para garantir consistência\ntrain_images, train_masks, train_fov_masks = load_data(TRAIN_IMG_DIR, TRAIN_MASK_DIR, TRAIN_FOV_DIR)\n\n# Separar treino e validação (incluindo o fov_masks agora)\nX_train, X_val, y_train, y_val, fov_train, fov_val = train_test_split(\n    train_images, train_masks, train_fov_masks, test_size=0.2, random_state=42 # Aumentei para 20% para uma validação mais robusta\n)\n\n# Criar patches para treinamento\ntrain_img_patches, train_mask_patches = create_patches(X_train, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:15:42.158207Z","iopub.execute_input":"2025-06-20T17:15:42.15896Z","iopub.status.idle":"2025-06-20T17:15:42.686337Z","shell.execute_reply.started":"2025-06-20T17:15:42.158929Z","shell.execute_reply":"2025-06-20T17:15:42.685575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carregar o melhor modelo salvo\nmodel = keras.models.load_model(\"unetpp_drive.keras\", custom_objects={\n    'bce_dice_loss': bce_dice_loss, \n    'dice_coefficient': dice_coefficient\n})\n\nprint(\"--- Iniciando Avaliação Quantitativa no Conjunto de VALIDAÇÃO ---\")\n\n# Esta linha CHAMA a função que definimos antes e calcula as métricas\nvalidation_metrics = evaluate_model(model, X_val, y_val, fov_val)\n\nprint(\"\\n--- Métricas Finais no Conjunto de Validação (média) ---\")\nfor metric, value in validation_metrics.items():\n    print(f\"{metric.capitalize()}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:15:45.871542Z","iopub.execute_input":"2025-06-20T17:15:45.871994Z","iopub.status.idle":"2025-06-20T17:15:54.011517Z","shell.execute_reply.started":"2025-06-20T17:15:45.87197Z","shell.execute_reply":"2025-06-20T17:15:54.010707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7.2. Visualização dos Resultados\nVisualizar as predições é a melhor forma de ter uma avaliação qualitativa.","metadata":{}},{"cell_type":"code","source":"# O objetivo é comparar a imagem, a máscara real e a predição.\n# Faremos isso no CONJUNTO DE VALIDAÇÃO, pois para ele temos as máscaras de gabarito.\n\nprint(\"\\n--- Iniciando Predição e Visualização Comparativa no Conjunto de VALIDAÇÃO ---\")\n\n# A função de plotagem permanece a mesma, ela já está correta.\n# Apenas vamos renomeá-la para maior clareza.\ndef plot_validation_predictions(model, images_to_plot, true_masks, num_samples=3):\n    \"\"\"\n    Plota a imagem original, a máscara real (gabarito) e a máscara predita.\n    \"\"\"\n    # Garante que não tentaremos plotar mais amostras do que as disponíveis\n    num_samples = min(num_samples, len(images_to_plot))\n\n    for i in range(num_samples):\n        img = images_to_plot[i]\n        true_mask = true_masks[i]\n\n        # Lógica para criar patches e prever\n        img_h, img_w = img.shape[0], img.shape[1]\n        patch_size = 128\n        stride = 64\n\n        test_patches_list = []\n        for y in range(0, img_h - patch_size + 1, stride):\n            for x in range(0, img_w - patch_size + 1, stride):\n                test_patches_list.append(img[y:y+patch_size, x:x+patch_size])\n\n        test_patches_np = np.array(test_patches_list)\n\n        if test_patches_np.shape[0] == 0:\n            print(f\"Nenhum patch gerado para a imagem de validação {i}. Pulando.\")\n            continue\n\n        # Fazer predições nos patches\n        if DEEP_SUPERVISION:\n            preds = model.predict(test_patches_np, verbose=0)[-1]\n        else:\n            preds = model.predict(test_patches_np, verbose=0)\n\n        # Reconstruir a máscara completa\n        pred_mask = reconstruct_from_patches(preds, img.shape, stride=stride)\n        pred_mask_bin = (pred_mask > 0.5).astype(np.uint8)\n\n        # Plotagem comparativa\n        plt.figure(figsize=(18, 6))\n\n        # Plot 1: Imagem de Validação Original\n        plt.subplot(1, 3, 1)\n        plt.title(f\"Imagem de Validação #{i+1}\")\n        plt.imshow(img)\n        plt.axis('off')\n\n        # Plot 2: Máscara Real (Gabarito)\n        plt.subplot(1, 3, 2)\n        plt.title(\"Máscara Real (Gabarito)\")\n        plt.imshow(true_mask.squeeze(), cmap='gray') # .squeeze() remove dimensões extras\n        plt.axis('off')\n\n        # Plot 3: Máscara Predita\n        plt.subplot(1, 3, 3)\n        plt.title(\"Máscara Predita pelo Modelo\")\n        plt.imshow(pred_mask_bin.squeeze(), cmap='gray')\n        plt.axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n# Chamamos a função passando os dados de VALIDAÇÃO (X_val, y_val)\nplot_validation_predictions(model, X_val, y_val, num_samples=6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:16:52.434232Z","iopub.execute_input":"2025-06-20T17:16:52.434509Z","iopub.status.idle":"2025-06-20T17:16:56.192792Z","shell.execute_reply.started":"2025-06-20T17:16:52.43449Z","shell.execute_reply":"2025-06-20T17:16:56.192099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# 'validation_metrics' é o dicionário que obtivemos na célula de avaliação\n# Se não o tiver mais, rode a avaliação novamente.\n\n# Criar um DataFrame do pandas a partir do dicionário\ndf_metrics = pd.DataFrame([validation_metrics])\n\n# Transpor a tabela para que as métricas fiquem nas linhas\ndf_metrics = df_metrics.T.reset_index()\n\n# Renomear as colunas para maior clareza\ndf_metrics.columns = ['Métrica', 'Valor']\n\n# Arredondar os valores para 4 casas decimais\ndf_metrics['Valor'] = df_metrics['Valor'].round(4)\n\nprint(\"Tabela de Desempenho do Modelo U-Net++ (no conjunto de validação)\")\ndisplay(df_metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:17:02.752116Z","iopub.execute_input":"2025-06-20T17:17:02.7529Z","iopub.status.idle":"2025-06-20T17:17:02.763172Z","shell.execute_reply.started":"2025-06-20T17:17:02.752872Z","shell.execute_reply":"2025-06-20T17:17:02.762436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Acessar os dados do histórico de treinamento\nhistory_dict = history.history\n\n# Perda (Loss)\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\n# Dice Coefficient (lembre-se que o nome da métrica é 'output_4_dice_coefficient' por causa da supervisão profunda)\ndice = history_dict['output_4_dice_coefficient']\nval_dice = history_dict['val_output_4_dice_coefficient']\n\nepochs = range(1, len(loss) + 1)\n\n# Criar a figura com dois subplots\nplt.figure(figsize=(14, 5))\n\n# Subplot 1: Gráfico de Perda\nplt.subplot(1, 2, 1)\nplt.plot(epochs, loss, 'bo-', label='Perda de Treino')\nplt.plot(epochs, val_loss, 'ro-', label='Perda de Validação')\nplt.title('Perda de Treino vs. Validação')\nplt.xlabel('Épocas')\nplt.ylabel('Perda (Loss)')\nplt.grid(True)\nplt.legend()\n\n# Subplot 2: Gráfico de Dice Coefficient\nplt.subplot(1, 2, 2)\nplt.plot(epochs, dice, 'bo-', label='Dice de Treino')\nplt.plot(epochs, val_dice, 'ro-', label='Dice de Validação')\nplt.title('Dice Coefficient de Treino vs. Validação')\nplt.xlabel('Épocas')\nplt.ylabel('Dice Coefficient')\nplt.grid(True)\nplt.legend()\n\nplt.suptitle('Evolução do Treinamento do Modelo', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:17:05.41704Z","iopub.execute_input":"2025-06-20T17:17:05.417534Z","iopub.status.idle":"2025-06-20T17:17:05.841439Z","shell.execute_reply.started":"2025-06-20T17:17:05.41751Z","shell.execute_reply":"2025-06-20T17:17:05.840771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\n# Vamos pegar uma imagem do conjunto de validação para gerar a curva\nimg_exemplo = X_val[0]\nmask_exemplo = y_val[0]\nfov_exemplo = fov_val[0]\n\n# Gerar patches da imagem de exemplo\nimg_h, img_w = img_exemplo.shape[0], img_exemplo.shape[1]\npatch_size = 128\nstride = 64\npatches_exemplo_list = []\nfor y in range(0, img_h - patch_size + 1, stride):\n    for x in range(0, img_w - patch_size + 1, stride):\n        patches_exemplo_list.append(img_exemplo[y:y+patch_size, x:x+patch_size])\npatches_exemplo_np = np.array(patches_exemplo_list)\n\n# Fazer a predição\nif DEEP_SUPERVISION:\n    preds_exemplo = model.predict(patches_exemplo_np, verbose=0)[-1]\nelse:\n    preds_exemplo = model.predict(patches_exemplo_np, verbose=0)\n\n# Reconstruir a predição\npred_mask_exemplo = reconstruct_from_patches(preds_exemplo, img_exemplo.shape, stride=stride)\n\n# Aplicar a máscara de FOV (campo de visão)\ntrue_labels = mask_exemplo.flatten()[fov_exemplo.flatten().astype(bool)]\npred_scores = pred_mask_exemplo.flatten()[fov_exemplo.flatten().astype(bool)]\n\n# Calcular a curva ROC\nfpr, tpr, thresholds = roc_curve(true_labels, pred_scores)\nroc_auc = auc(fpr, tpr)\n\n# Plotar o gráfico\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (área = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Classificador Aleatório')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Taxa de Falsos Positivos (1 - Especificidade)')\nplt.ylabel('Taxa de Verdadeiros Positivos (Sensibilidade)')\nplt.title('Curva ROC para uma Imagem de Exemplo')\nplt.grid(True)\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T17:17:19.078092Z","iopub.execute_input":"2025-06-20T17:17:19.078351Z","iopub.status.idle":"2025-06-20T17:17:19.750739Z","shell.execute_reply.started":"2025-06-20T17:17:19.078333Z","shell.execute_reply":"2025-06-20T17:17:19.750004Z"}},"outputs":[],"execution_count":null}]}